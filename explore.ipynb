{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"dataset/test.csv\")\n",
    "\n",
    "print('Training Set Shape = {}'.format(df_train.shape))\n",
    "print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n",
    "print('Test Set Shape = {}'.format(df_test.shape))\n",
    "print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train.describe())\n",
    "display(df_test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train.head())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sample(5).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_TRAINING_EXAMPLES = df_train.shape[0]\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.2\n",
    "STEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n",
    "\n",
    "EPOCHS = 2\n",
    "# Start with non-tensorflow approaches\n",
    "# AUTO = tf.data.experimental.AUTOTUNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test, validation split\n",
    "X = df_train[\"text\"]\n",
    "y = df_train[\"target\"]\n",
    "\n",
    "# If 42 is the answer, then 42*42 is the seed\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42*42)\n",
    "\n",
    "X_test = df_test[\"text\"] # Test data unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Vectorize the training dataset text\n",
    "# if it's fun to say, you know it's good\n",
    "# tiffeediff\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', norm='l2')\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train) # ALTERED: Vectorizing JUST the training data rather than the full dataset\n",
    "\n",
    "# Perform PCA analysis\n",
    "pca = PCA(n_components=4)\n",
    "X_train_pca = pca.fit_transform(X_train_vectorized.toarray())\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PCA components in a seaborn pairplot\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a dataframe from the PCA components\n",
    "df_pca = pd.DataFrame(X_train_pca, columns=['PCA1', 'PCA2', 'PCA3', 'PCA4'])\n",
    "df_pca['target'] = df_train['target']\n",
    "\n",
    "# Plot the pairplot\n",
    "sns.pairplot(df_pca, hue='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the words that are most correlated with the target variable\n",
    "\n",
    "# Create a dataframe from the vectorized text\n",
    "# sample = pd.DataFrame(X_train_vectorized.toarray()).sample(1000)\n",
    "df_vectorized = pd.DataFrame(X_train_vectorized.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# df_vectorized['target'] = df_train['target']\n",
    "\n",
    "df_vec_sample = df_vectorized.sample(5000)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "# correlation_matrix = df_vec_sample.corr()\n",
    "correlation_series = df_vec_sample.corrwith(df_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the words that are most correlated with the target variable\n",
    "# correlation_target = correlation_matrix['target']\n",
    "# correlation_target = correlation_target.drop('target')\n",
    "# correlation_target = correlation_target.sort_values(ascending=False).iloc[:10]\n",
    "\n",
    "# # Print the most correlated words\n",
    "# print(correlation_target)\n",
    "correlation_series.sort_values(ascending=False).iloc[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize and Dimensionality Reduce the Validation Data\n",
    "X_val_vectorized = vectorizer.transform(X_val)\n",
    "X_val_pca = pca.transform(X_val_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a basic SVC model\n",
    "svc = SVC()\n",
    "svc.fit(X_train_pca, y_train)\n",
    "y_pred = svc.predict(X_val_pca)\n",
    "\n",
    "# For a fancy SVC model, tune the hyperparameters of the SVC model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.logspace(-2, 2, 5),  # 0.01 to 100\n",
    "    'gamma': np.logspace(-4, 1, 6),  # 0.0001 to 10\n",
    "    # 'class_weight': [None, 'balanced']    # Uncomment if the dataset seems imbalanced (lots of 0s or 1s)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1)\n",
    "grid.fit(X_train_pca, y_train)\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# Train the new model with the best hyperparameters, compare to old SVC model\n",
    "svc_best = SVC(**grid.best_params_) # Double asterisk is to unpack best_params_ dictionary. DON'T REMOVE!\n",
    "svc_best.fit(X_train_pca, y_train)\n",
    "y_pred_best = svc_best.predict(X_val_pca)\n",
    "\n",
    "# Print both SVC accuracies\n",
    "print(\"Old SVC Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"New SVC Accuracy:\", accuracy_score(y_val, y_pred_best))\n",
    "\n",
    "# It's like 1% better, but it's better! And it likely generalizes to the test set better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
